"""
LLM Client for Attack Tree Generation

This module provides streaming interfaces to various LLM backends (OpenAI, Ollama)
for generating attack trees. All functions return async generators that yield
tokens in real-time as they're received from the LLM.

Supported backends:
- OpenAI (GPT models)
- Ollama (local models via OpenAI-compatible API)
"""
import os
from dotenv import load_dotenv

load_dotenv()

# Backend & model selection
LLM_BACKEND = os.getenv("LLM_BACKEND", "openai")  # "openai" or "ollama"
LLM_MODEL = os.getenv("LLM_MODEL")

# OpenAI configuration
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# Ollama configuration
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434/v1")

async def generate_attack_tree_stream_openai(prompt: str):
    """
    Stream attack tree generation from OpenAI's API.

    Args:
        prompt: The formatted prompt for attack tree generation

    Yields:
        str: Individual tokens as they're generated by the LLM

    Raises:
        RuntimeError: If OPENAI_API_KEY is not configured
    """
    from openai import AsyncOpenAI

    if not OPENAI_API_KEY:
        raise RuntimeError("OPENAI_API_KEY not configured")

    client = AsyncOpenAI(
        api_key=OPENAI_API_KEY,
        timeout=300.0  # 5 minute timeout
    )

    stream = await client.chat.completions.create(
        model=LLM_MODEL,
        messages=[{"role": "user", "content": prompt}],
        max_tokens=2000,
        temperature=0.2,
        stream=True
    )

    async for chunk in stream:
        if chunk.choices[0].delta.content:
            yield chunk.choices[0].delta.content


async def generate_attack_tree_stream_ollama(prompt: str):
    """
    Stream attack tree generation from Ollama via OpenAI-compatible API.

    Args:
        prompt: The formatted prompt for attack tree generation

    Yields:
        str: Individual tokens as they're generated by the LLM
    """
    from openai import AsyncOpenAI

    client = AsyncOpenAI(
        base_url=OLLAMA_BASE_URL,
        api_key="ollama",  # Ollama doesn't require a real API key
        timeout=300.0  # 5 minute timeout for slow models
    )

    stream = await client.chat.completions.create(
        model=LLM_MODEL,
        messages=[{"role": "user", "content": prompt}],
        max_tokens=2000,
        temperature=0.2,
        stream=True
    )

    async for chunk in stream:
        if chunk.choices[0].delta.content:
            yield chunk.choices[0].delta.content


async def generate_attack_tree_stream(prompt: str):
    """
    Stream attack tree generation from configured LLM backend.

    This function routes to the appropriate backend based on LLM_BACKEND
    environment variable.

    Args:
        prompt: The formatted prompt for attack tree generation

    Yields:
        str: Individual tokens as they're generated by the LLM

    Raises:
        RuntimeError: If LLM_BACKEND is not 'openai' or 'ollama'
    """
    backend = LLM_BACKEND.lower()

    if backend == "ollama":
        async for token in generate_attack_tree_stream_ollama(prompt):
            yield token
    elif backend == "openai":
        async for token in generate_attack_tree_stream_openai(prompt):
            yield token
    else:
        raise RuntimeError(f"Unsupported LLM_BACKEND: {backend}. Use 'openai' or 'ollama'")
